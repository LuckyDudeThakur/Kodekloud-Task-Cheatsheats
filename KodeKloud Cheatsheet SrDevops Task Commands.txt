Task 01 - Task 36 : Kodekloud Cheatsheat SysAdmin Task Commands.txt
Task 37 - Task 87 : Kodekloud Cheatsheat DevOps Task Commands.txt
--------------------------------------------------------------------------------------------------------------------------------
Task 88: 11/Oct/2022

Init Containers in Kubernetes

There are some applications that need to be deployed on Kubernetes cluster and these apps have some pre-requisites where some configurations need to be changed before deploying the app container. Some of these changes cannot be made inside the images so the DevOps team has come up with a solution to use init containers to perform these tasks during deployment. Below is a sample scenario that the team is going to test first.

    Create a Deployment named as ic-deploy-devops.

    Configure spec as replicas should be 1, labels app should be ic-devops, template's metadata lables app should be the same ic-devops.

    The initContainers should be named as ic-msg-devops, use image debian, preferably with latest tag and use command '/bin/bash', '-c' and 'echo Init Done - Welcome to xFusionCorp Industries > /ic/news'. The volume mount should be named as ic-volume-devops and mount path should be /ic.

    Main container should be named as ic-main-devops, use image debian, preferably with latest tag and use command '/bin/bash', '-c' and 'while true; do cat /ic/news; sleep 5; done'. The volume mount should be named as ic-volume-devops and mount path should be /ic.

    Volume to be named as ic-volume-devops and it should be an emptyDir type.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.


1. Check existing running services,deployment and pods
thor@jump_host ~$ kubectl get all
	NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
	service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   59m

thor@jump_host ~$ kubectl get services
	NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
	kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   59m

thor@jump_host ~$ kubectl get deploy
	No resources found in default namespace.

thor@jump_host ~$ kubectl get pods
	No resources found in default namespace.

2. Create yaml file as per requirement.
thor@jump_host ~$ vi /tmp/init.yml
 
thor@jump_host ~$ cat /tmp/init.yml 
		apiVersion: apps/v1

		kind: Deployment

		metadata:

		  name: ic-deploy-devops

		  labels:

		    app: ic-devops

		spec:

		  replicas: 1

		  selector:

		    matchLabels:

		      app: ic-devops

		  template:

		    metadata:

		      labels:

		        app: ic-devops

		    spec:

		      volumes:

		        - name: ic-volume-devops

		          emptyDir: {}

		      initContainers:

		        - name: ic-msg-devops

		          image: debian:latest

		          command:

		            [

		              "/bin/bash",

		              "-c",

		              "echo Init Done - Welcome to xFusionCorp Industries > /ic/news",

		            ]

		          volumeMounts:

		            - name: ic-volume-devops

		              mountPath: /ic

		 

		      containers:

		        - name: ic-main-devops

		          image: debian:latest

		          command:

		            [

		              "/bin/bash",

		              "-c",

		              "while true; do cat /ic/news; sleep 5; done",

		            ]

		          volumeMounts:

		            - name: ic-volume-devops

		              mountPath: /ic

3. Create pod and deployment 
thor@jump_host ~$ kubectl create -f /tmp/init.yml 
	deployment.apps/ic-deploy-devops created

4. Wait for pod to get to running status
thor@jump_host ~$ kubectl get pods -w
	NAME                                READY   STATUS    RESTARTS   AGE
	ic-deploy-devops-68f4dcbdfb-grg6v   1/1     Running   0          23s
	^C

thor@jump_host ~$ kubectl get deploy
	NAME               READY   UP-TO-DATE   AVAILABLE   AGE
	ic-deploy-devops   1/1     1            1           60s

thor@jump_host ~$ kubectl get pods
	NAME                                READY   STATUS    RESTARTS   AGE
	ic-deploy-devops-68f4dcbdfb-grg6v   1/1     Running   0          66s

5. Validate the task by checking logs on created pod and cat on created file
thor@jump_host ~$ kubectl logs -f ic-deploy-devops-68f4dcbdfb-grg6v
		Init Done - Welcome to xFusionCorp Industries
		Init Done - Welcome to xFusionCorp Industries
		Init Done - Welcome to xFusionCorp Industries
		Init Done - Welcome to xFusionCorp Industries
		Init Done - Welcome to xFusionCorp Industries
		Init Done - Welcome to xFusionCorp Industries
		Init Done - Welcome to xFusionCorp Industries
		^C

thor@jump_host ~$ kubectl exec ic-deploy-devops-68f4dcbdfb-grg6v -- cat /ic/news
		Init Done - Welcome to xFusionCorp Industries

--------------------------------------------------------------------------------------------------------------------------------
Task 89: 13/Oct/2022

Troubleshoot Issue With Pods

One of the junior DevOps team members was working on to deploy a stack on Kubernetes cluster. Somehow the pod is not coming up and its failing with some errors. We need to fix this as soon as possible. Please look into it.

    There is a pod named webserver and the container under it is named as nginx-container. It is using image nginx:latest

    There is a sidecar container as well named sidecar-container which is using ubuntu:latest image.

Look into the issue and fix it, make sure pod is in running state and you are able to access the app.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.



Name:         webserver
Namespace:    default
Priority:     0
Node:         kodekloud-control-plane/172.17.0.2
Start Time:   Thu, 13 Oct 2022 12:03:02 +0000
Labels:       app=web-app
Annotations:  <none>
Status:       Pending
IP:           10.244.0.5
IPs:
  IP:  10.244.0.5
Containers:
  nginx-container:
    Container ID:   
    Image:          nginx:latests
    Image ID:       
    Port:           <none>
    Host Port:      <none>
    State:          Waiting
      Reason:       ImagePullBackOff
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/log/nginx from shared-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-2dgf4 (ro)
  sidecar-container:
    Container ID:  containerd://f186ca761ed3228d82850c85ff4fbf29b954ee664214699ac609288f80cfe75c
    Image:         ubuntu:latest
    Image ID:      docker.io/library/ubuntu@sha256:35fb073f9e56eb84041b0745cb714eff0f7b225ea9e024f703cab56aaa5c7720
    Port:          <none>
    Host Port:     <none>
    Command:
      sh
      -c
      while true; do cat /var/log/nginx/access.log /var/log/nginx/error.log; sleep 30; done
    State:          Running
      Started:      Thu, 13 Oct 2022 12:03:10 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/log/nginx from shared-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-2dgf4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  shared-logs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  default-token-2dgf4:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-2dgf4
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  61s                default-scheduler  Successfully assigned default/webserver to kodekloud-control-plane
  Normal   Pulling    60s                kubelet            Pulling image "ubuntu:latest"
  Normal   Started    53s                kubelet            Started container sidecar-container
  Normal   Pulled     53s                kubelet            Successfully pulled image "ubuntu:latest" in 6.384820926s
  Normal   Created    53s                kubelet            Created container sidecar-container
  Normal   BackOff    26s (x3 over 53s)  kubelet            Back-off pulling image "nginx:latests"
  Warning  Failed     26s (x3 over 53s)  kubelet            Error: ImagePullBackOff
  Normal   Pulling    12s (x3 over 60s)  kubelet            Pulling image "nginx:latests"
  Warning  Failed     11s (x3 over 60s)  kubelet            Failed to pull image "nginx:latests": rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/nginx:latests": failed to resolve reference "docker.io/library/nginx:latests": docker.io/library/nginx:latests: not found
  Warning  Failed     11s (x3 over 60s)  kubelet            Error: ErrImagePull


1. Check pod running status using kubectl utility
thor@jump_host ~$ kubectl get pods
	NAME        READY   STATUS             RESTARTS   AGE
	webserver   1/2     ImagePullBackOff   0          30s

thor@jump_host ~$ kubectl get pods webserver
	NAME        READY   STATUS         RESTARTS   AGE
	webserver   1/2     ErrImagePull   0          39s

2. Check pod configuration and try to indentify error
thor@jump_host ~$ kubectl describe pod webserver
		Name:         webserver
		Namespace:    default
		Priority:     0
		Node:         kodekloud-control-plane/172.17.0.2
		Start Time:   Thu, 13 Oct 2022 12:03:02 +0000
		Labels:       app=web-app
		Annotations:  <none>
		Status:       Pending
		IP:           10.244.0.5
		IPs:
		  IP:  10.244.0.5
		Containers:
		|------------------------------------------------------------------------------------------------------------------------
		| nginx-container:
		|   Container ID:   
		|   Image:          nginx:latests											<-------- Error in container image name
		|   Image ID:       
		|   Port:           <none>
		|   Host Port:      <none>
		|   State:          Waiting
		|     Reason:       ImagePullBackOff
		|   Ready:          False
		|   Restart Count:  0
		|   Environment:    <none>
		|   Mounts:
		|     /var/log/nginx from shared-logs (rw)
		|     /var/run/secrets/kubernetes.io/serviceaccount from default-token-2dgf4 (ro)
		|------------------------------------------------------------------------------------------------------------------------      
		  sidecar-container:
		    Container ID:  containerd://f186ca761ed3228d82850c85ff4fbf29b954ee664214699ac609288f80cfe75c
		    Image:         ubuntu:latest
		    Image ID:      docker.io/library/ubuntu@sha256:35fb073f9e56eb84041b0745cb714eff0f7b225ea9e024f703cab56aaa5c7720
		    Port:          <none>
		    Host Port:     <none>
		    Command:
		      sh
		      -c
		      while true; do cat /var/log/nginx/access.log /var/log/nginx/error.log; sleep 30; done
		    State:          Running
		      Started:      Thu, 13 Oct 2022 12:03:10 +0000
		    Ready:          True
		    Restart Count:  0
		    Environment:    <none>
		    Mounts:
		      /var/log/nginx from shared-logs (rw)
		      /var/run/secrets/kubernetes.io/serviceaccount from default-token-2dgf4 (ro)
		Conditions:
		  Type              Status
		  Initialized       True 
		  Ready             False 
		  ContainersReady   False 
		  PodScheduled      True 
		Volumes:
		  shared-logs:
		    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
		    Medium:     
		    SizeLimit:  <unset>
		  default-token-2dgf4:
		    Type:        Secret (a volume populated by a Secret)
		    SecretName:  default-token-2dgf4
		    Optional:    false
		QoS Class:       BestEffort
		Node-Selectors:  <none>
		Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
		                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
		Events:
		  Type     Reason     Age                From               Message
		  ----     ------     ----               ----               -------
		  Normal   Scheduled  61s                default-scheduler  Successfully assigned default/webserver to kodekloud-control-plane
		  Normal   Pulling    60s                kubelet            Pulling image "ubuntu:latest"
		  Normal   Started    53s                kubelet            Started container sidecar-container
		  Normal   Pulled     53s                kubelet            Successfully pulled image "ubuntu:latest" in 6.384820926s
		  Normal   Created    53s                kubelet            Created container sidecar-container
		  Normal   BackOff    26s (x3 over 53s)  kubelet            Back-off pulling image "nginx:latests"
		  Warning  Failed     26s (x3 over 53s)  kubelet            Error: ImagePullBackOff
		  Normal   Pulling    12s (x3 over 60s)  kubelet            Pulling image "nginx:latests"
		  Warning  Failed     11s (x3 over 60s)  kubelet            Failed to pull image "nginx:latests": rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/nginx:latests": failed to resolve reference "docker.io/library/nginx:latests": docker.io/library/nginx:latests: not found
		  Warning  Failed     11s (x3 over 60s)  kubelet            Error: ErrImagePull


3. Edit the pod to change the container image name (using vi editor or default editor)
thor@jump_host ~$ kubectl edit pod webserver
		pod/webserver edited

4. Wait for pod to running status
thor@jump_host ~$ kubectl get pod webserver
		NAME        READY   STATUS    RESTARTS   AGE
		webserver   2/2     Running   0          2m22s

thor@jump_host ~$ kubectl describe pod webserver
		Name:         webserver
		Namespace:    default
		Priority:     0
		Node:         kodekloud-control-plane/172.17.0.2
		Start Time:   Thu, 13 Oct 2022 12:03:02 +0000
		Labels:       app=web-app
		Annotations:  <none>
		Status:       Running
		IP:           10.244.0.5
		IPs:
		  IP:  10.244.0.5
		Containers:
		|------------------------------------------------------------------------------------------------------------------------
		| nginx-container:																										
		|   Container ID:   containerd://96e18a2d97d0dcd78289713a2656e1150b4615dd6139fc56c293bc283e147616							
		|   Image:          nginx:latest
		|   Image ID:       docker.io/library/nginx@sha256:2f770d2fe27bc85f68fd7fe6a63900ef7076bc703022fe81b980377fe3d27b70
		|   Port:           <none>
		|   Host Port:      <none>
		|   State:          Running																		<------------Error resolved, pod running
		|     Started:      Thu, 13 Oct 2022 12:05:22 +0000
		|   Ready:          True
		|   Restart Count:  0
		|   Environment:    <none>
		|   Mounts:
		|     /var/log/nginx from shared-logs (rw)
		|     /var/run/secrets/kubernetes.io/serviceaccount from default-token-2dgf4 (ro)
		|------------------------------------------------------------------------------------------------------------------------|
		  sidecar-container:
		    Container ID:  containerd://f186ca761ed3228d82850c85ff4fbf29b954ee664214699ac609288f80cfe75c
		    Image:         ubuntu:latest
		    Image ID:      docker.io/library/ubuntu@sha256:35fb073f9e56eb84041b0745cb714eff0f7b225ea9e024f703cab56aaa5c7720
		    Port:          <none>
		    Host Port:     <none>
		    Command:
		      sh
		      -c
		      while true; do cat /var/log/nginx/access.log /var/log/nginx/error.log; sleep 30; done
		    State:          Running
		      Started:      Thu, 13 Oct 2022 12:03:10 +0000
		    Ready:          True
		    Restart Count:  0
		    Environment:    <none>
		    Mounts:
		      /var/log/nginx from shared-logs (rw)
		      /var/run/secrets/kubernetes.io/serviceaccount from default-token-2dgf4 (ro)
		Conditions:
		  Type              Status
		  Initialized       True 
		  Ready             True 
		  ContainersReady   True 
		  PodScheduled      True 
		Volumes:
		  shared-logs:
		    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
		    Medium:     
		    SizeLimit:  <unset>
		  default-token-2dgf4:
		    Type:        Secret (a volume populated by a Secret)
		    SecretName:  default-token-2dgf4
		    Optional:    false
		QoS Class:       BestEffort
		Node-Selectors:  <none>
		Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
		                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
		Events:
		  Type     Reason     Age                  From               Message
		  ----     ------     ----                 ----               -------
		  Normal   Scheduled  2m28s                default-scheduler  Successfully assigned default/webserver to kodekloud-control-plane
		  Normal   Pulling    2m27s                kubelet            Pulling image "ubuntu:latest"
		  Normal   Started    2m20s                kubelet            Started container sidecar-container
		  Normal   Pulled     2m20s                kubelet            Successfully pulled image "ubuntu:latest" in 6.384820926s
		  Normal   Created    2m20s                kubelet            Created container sidecar-container
		  Normal   Pulling    99s (x3 over 2m27s)  kubelet            Pulling image "nginx:latests"
		  Warning  Failed     98s (x3 over 2m27s)  kubelet            Failed to pull image "nginx:latests": rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/nginx:latests": failed to resolve reference "docker.io/library/nginx:latests": docker.io/library/nginx:latests: not found
		  Warning  Failed     98s (x3 over 2m27s)  kubelet            Error: ErrImagePull
		  Normal   BackOff    62s (x6 over 2m20s)  kubelet            Back-off pulling image "nginx:latests"
		  Warning  Failed     62s (x6 over 2m20s)  kubelet            Error: ImagePullBackOff

thor@jump_host ~$ kubectl get pods
	NAME        READY   STATUS    RESTARTS   AGE
	webserver   2/2     Running   0          3m46s

--------------------------------------------------------------------------------------------------------------------------------
Task 90: 15/Oct/2022

Persistent Volumes in Kubernetes HTTPD

The Nautilus DevOps team is working on a Kubernetes template to deploy a web application on the cluster. There are some requirements to create/use persistent volumes to store the application code, and the template needs to be designed accordingly. Please find more details below:

    Create a PersistentVolume named as pv-datacenter. Configure the spec as storage class should be manual, set capacity to 5Gi, set access mode to ReadWriteOnce, volume type should be hostPath and set path to /mnt/itadmin (this directory is already created, you might not be able to access it directly, so you need not to worry about it).

    Create a PersistentVolumeClaim named as pvc-datacenter. Configure the spec as storage class should be manual, request 1Gi of the storage, set access mode to ReadWriteOnce.

    Create a pod named as pod-datacenter, mount the persistent volume you created with claim name pvc-datacenter at document root of the web server, the container within the pod should be named as container-datacenter using image httpd with latest tag only (remember to mention the tag i.e httpd:latest).

    Create a node port type service named web-datacenter using node port 30008 to expose the web server running within the pod.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

1. Check kubectl utility for currently running services and pods
thor@jump_host ~$ kubectl get all
	NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
	service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   65m
 
thor@jump_host ~$ kubectl get pvc
	No resources found in default namespace.

2. Create YAML file as per requirements 
thor@jump_host ~$ vi /tmp/pvc_httpd.yml

thor@jump_host ~$ cat /tmp/pvc_httpd.yml 
		---
		apiVersion: v1
		kind: PersistentVolume
		metadata:
		  name: pv-datacenter
		spec:
		  capacity:
		    storage: 5Gi
		  accessModes:
		    - ReadWriteOnce
		  storageClassName: manual
		  hostPath:
		    path: /mnt/itadmin
		---
		apiVersion: v1
		kind: PersistentVolumeClaim
		metadata:
		  name: pvc-datacenter
		spec:
		  accessModes:
		    - ReadWriteOnce
		  storageClassName: manual
		  resources:
		    requests:
		      storage: 1Gi
		---
		apiVersion: v1
		kind: Pod
		metadata:
		  name: pod-datacenter
		  labels:
		     app: httpd
		spec:
		  volumes:
		    - name: storage-datacenter
		      persistentVolumeClaim:
		        claimName: pvc-datacenter
		  containers:
		    - name: container-datacenter
		      image: httpd:latest
		      ports:
		        - containerPort: 80
		      volumeMounts:
		        - name: storage-datacenter
		          mountPath:  /usr/local/apache2/htdocs/
		---                                                                                                           
		apiVersion: v1                                                                                                
		kind: Service                                                                                                 
		metadata:                                                                                                     
		  name: web-datacenter                                                                                         
		spec:                                                                                                         
		   type: NodePort                                                                                             
		   selector:                                                                                                  
		     app: httpd                                                                                     
		   ports:                                                                                                     
		     - port: 80                                                                                               
		       targetPort: 80                                                                                         
		       nodePort: 30008

3. Create the deployment , pods and persistent volumes
thor@jump_host ~$ kubectl create -f /tmp/pvc_httpd.yml 
	persistentvolume/pv-datacenter created
	persistentvolumeclaim/pvc-datacenter created
	pod/pod-datacenter created
	service/web-datacenter created

4. Wait for pods and services to running status
thor@jump_host ~$ kubectl get all
		NAME                 READY   STATUS    RESTARTS   AGE
		pod/pod-datacenter   0/1     Pending   0          6s

		NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
		service/kubernetes       ClusterIP   10.96.0.1       <none>        443/TCP        72m
		service/web-datacenter   NodePort    10.96.147.100   <none>        80:30008/TCP   6s

 
thor@jump_host ~$ kubectl get all
	NAME                 READY   STATUS    RESTARTS   AGE
	pod/pod-datacenter   1/1     Running   0          30s

	NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
	service/kubernetes       ClusterIP   10.96.0.1       <none>        443/TCP        73m
	service/web-datacenter   NodePort    10.96.147.100   <none>        80:30008/TCP   30s

5. Validate Psersistent Volume Claim and Peristent volume 
thor@jump_host ~$ kubectl get pvc
	NAME             STATUS   VOLUME          CAPACITY   ACCESS MODES   STORAGECLASS   AGE
	pvc-datacenter   Bound    pv-datacenter   5Gi        RWO            manual         40s
 
thor@jump_host ~$ kubectl get pv
	NAME            CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS   REASON   AGE
	pv-datacenter   5Gi        RWO            Retain           Bound    default/pvc-datacenter   manual                  45s

6. Validate task by View port on website

--------------------------------------------------------------------------------------------------------------------------------
Task 91: 17/Oct/2022

Resolve Git Merge Conflicts

Sarah and Max were working on writting some stories which they have pushed to the repository. Max has recently added some new changes and is trying to push them to the repository but he is facing some issues. Below you can find more details:

SSH into storage server using user max and password Max_pass123. Under /home/max you will find the story-blog repository. Try to push the changes to the origin repo and fix the issues. The story-index.txt must have titles for all 4 stories. Additionally, there is a typo in The Lion and the Mooose line where Mooose should be Mouse.

Click on the Gitea UI button on the top bar. You should be able to access the Gitea page. You can login to Gitea server from UI using username sarah and password Sarah_pass123 or username max and password Max_pass123.

Note: For these kind of scenarios requiring changes to be done in a web UI, please take screenshots so that you can share it with us for review in case your task is marked incomplete. You may also consider using a screen recording software such as loom.com to record and share your work.

1. SSH to storage server with user max
thor@jump_host ~$ ssh max@ststor01
		The authenticity of host 'ststor01 (172.16.238.15)' can't be established.
		ECDSA key fingerprint is SHA256:0z85j/k+4Nf8WKbHJzxo1AOv4FeRA8LPET2N3BEkYyo.
		ECDSA key fingerprint is MD5:74:e6:4d:c4:b3:80:07:be:03:30:0a:bf:1e:eb:e6:82.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'ststor01,172.16.238.15' (ECDSA) to the list of known hosts.
max@ststor01's password: 
		Welcome to xFusionCorp Storage server.

2. Go to local git repo and check git status
max $ cd /home/max/story-blog/

max (master)$ ls -ahl
		total 32
		drwxr-sr-x    3 max      max         4.0K Oct 17 14:06 .
		drwxr-sr-x    1 max      max         4.0K Oct 17 14:06 ..
		drwxr-sr-x    8 max      max         4.0K Oct 17 14:06 .git
		-rw-r--r--    1 max      max          807 Oct 17 14:06 fox-and-grapes.txt
		-rw-r--r--    1 max      max          792 Oct 17 14:06 frogs-and-ox.txt
		-rw-r--r--    1 max      max         1.1K Oct 17 14:06 lion-and-mouse.txt
		-rw-r--r--    1 max      max          102 Oct 17 14:06 story-index.txt

max (master)$ git status
		On branch master
		Your branch is ahead of 'origin/master' by 1 commit.
		  (use "git push" to publish your local commits)
		nothing to commit, working directory clean
 
3.Check story-index.txt file
max (master)$ cat story-index.txt 
		1. The Lion and the Mooose
		2. The Frogs and the Ox
		3. The Fox and the Grapes
		4. The Donkey and the Dog

4. Try to push the changes to know the error		
max (master)$ git push
		Username for 'http://git.stratos.xfusioncorp.com': max
		Password for 'http://max@git.stratos.xfusioncorp.com': 
		To http://git.stratos.xfusioncorp.com/sarah/story-blog.git
		 ! [rejected]        master -> master (fetch first)
		error: failed to push some refs to 'http://git.stratos.xfusioncorp.com/sarah/story-blog.git'
		hint: Updates were rejected because the remote contains work that you do
		hint: not have locally. This is usually caused by another repository pushing
		hint: to the same ref. You may want to first integrate the remote changes
		hint: (e.g., 'git pull ...') before pushing again.
		hint: See the 'Note about fast-forwards' in 'git push --help' for details.

5. Conflict in between local repo and Gitex repo. Setglobal variables 

max (master)$ git config --global --add user.email max@stratos.xfusioncorp.com

max (master)$ git config --global --add user.name max

max (master)$ git config -l
	user.email=max@stratos.xfusioncorp.com
	user.name=max
	core.repositoryformatversion=0
	core.filemode=true
	core.bare=false
	core.logallrefupdates=true
	remote.origin.url=http://git.stratos.xfusioncorp.com/sarah/story-blog.git
	remote.origin.fetch=+refs/heads/*:refs/remotes/origin/*
	branch.master.remote=origin
	branch.master.merge=refs/heads/master

6. Pull the changes from gitex to local 
max (master)$ git  pull origin master
	remote: Enumerating objects: 4, done.
	remote: Counting objects: 100% (4/4), done.
	remote: Compressing objects: 100% (3/3), done.
	remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0
	Unpacking objects: 100% (3/3), done.
	From http://git.stratos.xfusioncorp.com/sarah/story-blog
	 * branch            master     -> FETCH_HEAD
	   9f02757..7edc0ea  master     -> origin/master
	Auto-merging story-index.txt
	CONFLICT (add/add): Merge conflict in story-index.txt
	Automatic merge failed; fix conflicts and then commit the result.

7. Conflict while merging story-index.txt. Edit and clear those conflict

max (master)$ vi story-index.txt

max (master)$ cat story-index.txt 
	1. The Lion and the Mouse
	2. The Frogs and the Ox
	3. The Fox and the Grapes
	4. The Donkey and the Dog

8. Push the changes back to remote repository

max (master)$ git add story-index.txt 

max (master)$ git commit -m "Fixed the issue"
	[master cf96f89] Fixed the issue
	 1 file changed, 6 deletions(-)

max (master)$ git push origin  master
	Username for 'http://git.stratos.xfusioncorp.com': max
	Password for 'http://max@git.stratos.xfusioncorp.com': 
	Counting objects: 3, done.
	Delta compression using up to 36 threads.
	Compressing objects: 100% (3/3), done.
	Writing objects: 100% (3/3), 280 bytes | 0 bytes/s, done.
	Total 3 (delta 2), reused 0 (delta 0)
	remote: . Processing 1 references
	remote: Processed 1 references in total
	To http://git.stratos.xfusioncorp.com/sarah/story-blog.git
	   acefab2..cf96f89  master -> master
 
max (master)$ git status
	On branch master
	Your branch is up-to-date with 'origin/master'.
	nothing to commit, working directory clean

max (master)$ git pull origin  master
	From http://git.stratos.xfusioncorp.com/sarah/story-blog
	 * branch            master     -> FETCH_HEAD
	Already up-to-date.
 
max (master)$ git status
	On branch master
	Your branch is up-to-date with 'origin/master'.
	nothing to commit, working directory clean

max (master)$ 

--------------------------------------------------------------------------------------------------------------------------------
Task 92: 19/Oct/2022

Managing Jinja2 Templates Using Ansible

One of the Nautilus DevOps team members is working on to develop a role for httpd installation and configuration. Work is almost completed, however there is a requirement to add a jinja2 template for index.html file. Additionally, the relevant task needs to be added inside the role. The inventory file ~/ansible/inventory is already present on jump host that can be used. Complete the task as per details mentioned below:

a. Update ~/ansible/playbook.yml playbook to run the httpd role on App Server 1.

b. Create a jinja2 template index.html.j2 under /home/thor/ansible/role/httpd/templates/ directory and add a line This file was created using Ansible on <respective server> (for example This file was created using Ansible on stapp01 in case of App Server 1). Also please make sure not to hard code the server name inside the template. Instead, use inventory_hostname variable to fetch the correct value.

c. Add a task inside /home/thor/ansible/role/httpd/tasks/main.yml to copy this template on App Server 1 under /var/www/html/index.html. Also make sure that /var/www/html/index.html file's permissions are 0777.

d. The user/group owner of /var/www/html/index.html file must be respective sudo user of the server (for example tony in case of stapp01).

Note: Validation will try to run the playbook using command ansible-playbook -i inventory playbook.yml so please make sure the playbook works this way without passing any extra arguments.


1. Go to mentioned folder and verify playbook and inventory

thor@jump_host ~$ cd /home/thor/ansible/

thor@jump_host ~/ansible$ ls -ahl
		total 20K
		drwxr-xr-x 3 thor thor 4.0K Oct 19 05:55 .
		drwxr----- 1 thor thor 4.0K Oct 19 05:55 ..
		-rw-r--r-- 1 thor thor  237 Oct 19 05:55 inventory
		-rw-r--r-- 1 thor thor   73 Oct 19 05:55 playbook.yml
		drwxr-xr-x 3 thor thor 4.0K Oct 19 05:55 role

thor@jump_host ~/ansible$ cat inventory 
		stapp01 ansible_host=172.16.238.10 ansible_user=tony ansible_ssh_pass=Ir0nM@n
		stapp02 ansible_host=172.16.238.11 ansible_user=steve ansible_ssh_pass=Am3ric@
		stapp03 ansible_host=172.16.238.12 ansible_user=banner ansible_ssh_pass=BigGr33n

 
thor@jump_host ~/ansible$ cat playbook.yml 
		---
		- hosts: 
		  become: yes
		  become_user: root
		  roles:
		    - role/httpd

2. Edit the playbook to add the mentioned host (stapp01)

thor@jump_host ~/ansible$ vi playbook.yml 

thor@jump_host ~/ansible$ cat playbook.yml 
		---
		- hosts: stapp01						<--------------------------
		  become: yes
		  become_user: root
		  roles:
		    - role/httpd

3. Create Jinja2 template with mentioned content

thor@jump_host ~/ansible$ vi /home/thor/ansible/role/httpd/templates/index.html.j2

thor@jump_host ~/ansible$ cat /home/thor/ansible/role/httpd/templates/index.html.j2
		This file was created using Ansible on {{ ansible_hostname }}


4. Edit the mian.yml file to add a task to copy the Jinja2 template  

thor@jump_host ~/ansible$ vi /home/thor/ansible/role/httpd/tasks/main.yml

thor@jump_host ~/ansible$ cat /home/thor/ansible/role/httpd/tasks/main.yml
		---
		# tasks file for role/test

		- name: install the latest version of HTTPD
		  yum:
		    name: httpd
		    state: latest

		- name: Start service httpd
		  service:
		    name: httpd
		    state: started

		- name: Use Jinja2 template to generate index.html
		  template:
		    src: /home/thor/ansible/role/httpd/templates/index.html.j2
		    dest: /var/www/html/index.html
		    mode: "0777"
		    owner: "{{ ansible_user }}"
		    group: "{{ ansible_user }}"


5. Run the playbook

thor@jump_host ~/ansible$ ansible-playbook -i inventory playbook.yml 

		PLAY [stapp01] ******************************************************************************************************************************

		TASK [Gathering Facts] **********************************************************************************************************************
		ok: [stapp01]

		TASK [role/httpd : install the latest version of HTTPD] *************************************************************************************
		changed: [stapp01]

		TASK [role/httpd : Start service httpd] *****************************************************************************************************
		changed: [stapp01]

		TASK [role/httpd : Use Jinja2 template to generate index.html] ******************************************************************************
		changed: [stapp01]

		PLAY RECAP **********************************************************************************************************************************
		stapp01                    : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   


6. Validate the changes on app server

thor@jump_host ~/ansible$ ansible stapp01 -i inventory  -a "cat /var/www/html/index.html"
		stapp01 | CHANGED | rc=0 >>
		This file was created using Ansible on stapp01

thor@jump_host ~/ansible$ ansible stapp01 -i inventory  -a "ls -ahl /var/www/html/index.html"
		stapp01 | CHANGED | rc=0 >>
		-rwxrwxrwx 1 tony tony 47 Oct 19 06:03 /var/www/html/index.html

thor@jump_host ~/ansible$ ansible stapp02 -i inventory  -a "cat /var/www/html/index.html"
		stapp02 | FAILED | rc=1 >>
		cat: /var/www/html/index.html: No such file or directorynon-zero return code

thor@jump_host ~/ansible$ ansible stapp03 -i inventory  -a "cat /var/www/html/index.html"
		stapp03 | FAILED | rc=1 >>
		cat: /var/www/html/index.html: No such file or directorynon-zero return code

--------------------------------------------------------------------------------------------------------------------------------
Task 93: 20/Oct/2022

Ansible Inventory Update

The Nautilus DevOps team has started testing their Ansible playbooks on different servers within the stack. They have placed some playbooks under /home/thor/playbook/ directory on jump host which they want to test. Some of these playbooks have already been tested on different servers, but now they want to test them on app server 3 in Stratos DC. However, they first need to create an inventory file so that Ansible can connect to the respective app. Below are some requirements:

a. Create an ini type Ansible inventory file /home/thor/playbook/inventory on jump host.

b. Add App Server 3 in this inventory along with required variables that are needed to make it work.

c. The inventory hostname of the host should be the server name as per the wiki, for example stapp01 for app server 1 in Stratos DC.

Note: Validation will try to run the playbook using command ansible-playbook -i inventory playbook.yml so please make sure the playbook works this way without passing any extra arguments.


1. Verify mentioned playbook and other files 

thor@jump_host ~$ cd /home/thor/playbook/

thor@jump_host ~/playbook$ ls -ahl
		total 16K
		drwxr-xr-x 2 thor thor 4.0K Oct 20 09:36 .
		drwxr----- 1 thor thor 4.0K Oct 20 09:36 ..
		-rw-r--r-- 1 thor thor   36 Oct 20 09:36 ansible.cfg
		-rw-r--r-- 1 thor thor  250 Oct 20 09:36 playbook.yml

thor@jump_host ~/playbook$ cat playbook.yml 
		---
		- hosts: all
		  become: yes
		  become_user: root
		  tasks:
		    - name: Install httpd package    
		      yum: 
		        name: httpd 
		        state: installed
		    
		    - name: Start service httpd
		      service:
		        name: httpd
		        state: started

2. Create the mentioned inventory file with App Server 3 details

thor@jump_host ~/playbook$ vi /home/thor/playbook/inventory

thor@jump_host ~/playbook$ cat /home/thor/playbook/inventory
		stapp03 ansible_host=172.16.238.12 ansible_ssh_pass=BigGr33n  ansible_user=banner

3. Exectue the playbook and verify.

thor@jump_host ~/playbook$ ansible-playbook -i inventory playbook.yml

		PLAY [all] **********************************************************************************************************************************

		TASK [Gathering Facts] **********************************************************************************************************************
		ok: [stapp03]

		TASK [Install httpd package] ****************************************************************************************************************
		changed: [stapp03]

		TASK [Start service httpd] ******************************************************************************************************************
		changed: [stapp03]

		PLAY RECAP **********************************************************************************************************************************
		stapp03                    : ok=3    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

thor@jump_host ~/playbook$

--------------------------------------------------------------------------------------------------------------------------------
Task 94: 22/Oct/2022

Git Fork a Repository


There is a Git server used by the Nautilus project teams. Recently a new developer Jon joined the team and needs to start working on a project. To do so, he needs to fork an existing Git repository. Below you can find more details:

    Click on the Gitea UI button on the top bar. You should be able to access the Gitea page.

    Login to Gitea server using username jon and password Jon_pass123.

    There you will see a Git repository sarah/story-blog, fork it under jon user.

Note: For these kind of scenarios requiring changes to be done in a web UI, please take screenshots so that you can share it with us for review in case your task is marked incomplete. You may also consider using a screen recording software such as loom.com to record and share your work.


1. As mentioned open Gitea UI

2. Login to Gitea server using username jon and password Jon_pass123

3. Go to repo sarah/story-blog

4. Right top corner option for fork the repo

5. The new forked repo will be under jon/story-blog

--------------------------------------------------------------------------------------------------------------------------------
Task 95: 23/Oct/2022

Create a Docker Image From Container

One of the Nautilus developer was working to test new changes on a container. He wants to keep a backup of his changes to the container. A new request has been raised for the DevOps team to create a new image from this container. Below are more details about it:

a. Create an image apps:xfusion on Application Server 2 from a container ubuntu_latest that is running on same server.


1. 1. Login on app server  & Switch to  root user

thor@jump_host ~$ ssh steve@stapp02
		The authenticity of host 'stapp02 (172.16.238.11)' can't be established.
		ECDSA key fingerprint is SHA256:LmeMEQk6Mx7vqZWW6o6Knsvsgqwb4FlOk7e/cSvtfms.
		ECDSA key fingerprint is MD5:b8:e8:31:0f:29:8b:c7:be:26:6e:42:aa:56:51:29:8c.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp02,172.16.238.11' (ECDSA) to the list of known hosts.
		steve@stapp02's password: 

[steve@stapp02 ~]$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for steve: 


2. Check existing docker container ubuntu_latest running status 

[root@stapp02 ~]# docker ps 
		CONTAINER ID   IMAGE     COMMAND   CREATED              STATUS              PORTS     NAMES
		e2d2e33ed57c   ubuntu    "bash"    About a minute ago   Up About a minute             ubuntu_latest

3. Check current docker images on your system

[root@stapp02 ~]# docker images
		REPOSITORY   TAG       IMAGE ID       CREATED       SIZE
		ubuntu       latest    216c552ea5ba   2 weeks ago   77.8MB

4. Create new image from given contianer as per task

[root@stapp02 ~]# docker commit ubuntu_latest apps:xfusion
		sha256:2158e1542b17e48950a0c1fd45ab50729fbd9dd344210b5771b0113955100168

5. Verify and validate the new  docker image created on your system

[root@stapp02 ~]# docker images
		REPOSITORY   TAG       IMAGE ID       CREATED         SIZE
		apps         xfusion   2158e1542b17   6 seconds ago   116MB
		ubuntu       latest    216c552ea5ba   2 weeks ago     77.8MB

--------------------------------------------------------------------------------------------------------------------------------
Task 96: 25/Oct/2022

Puppet Multi-Packages Installation

Some new changes need to be made on some of the app servers in Stratos Datacenter. There are some packages that need to be installed on the app server 2. We want to install these packages using puppet only.

    Puppet master is already installed on Jump Server.

    Create a puppet programming file blog.pp under /etc/puppetlabs/code/environments/production/manifests on master node i.e on Jump Server and perform below mentioned tasks using the same.

    Define a class multi_package_node for agent node 2 i.e app server 2. Install net-tools and unzip packages on the agent node 2.

Notes: :- Please make sure to run the puppet agent test using sudo on agent nodes, otherwise you can face certificate issues. In that case you will have to clean the certificates first and then you will be able to run the puppet agent test.

:- Before clicking on the Check button please make sure to verify puppet server and puppet agent services are up and running on the respective servers, also please make sure to run puppet agent test to apply/test the changes manually first.

:- Please note that once lab is loaded, the puppet server service should start automatically on puppet master server, however it can take upto 2-3 minutes to start.


1. Switch to root user on jump host to create puppet programming file

thor@jump_host ~$ sudo su - 

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for thor: 

2. Go to mentione folder location and create blog.pp  file with given requirements

root@jump_host ~# cd /etc/puppetlabs/code/environments/production/manifests/
 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# ls -ahl
		total 8.0K
		drwxr-xr-x 1 puppet puppet 4.0K Jul 13  2021 .
		drwxr-xr-x 1 puppet puppet 4.0K Aug  9  2021 ..
 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# vi blog.pp

root@jump_host /etc/puppetlabs/code/environments/production/manifests# cat blog.pp 
		class multi_package_node {
		$multi_package = [ 'net-tools', 'unzip']
		    package { $multi_package: ensure => 'installed' }
		}

		node 'stapp02.stratos.xfusioncorp.com' {
		  include multi_package_node
		}

3. Login to ap server 2 and switch to root user

root@jump_host /etc/puppetlabs/code/environments/production/manifests# ssh steve@stapp02
		The authenticity of host 'stapp02 (172.16.238.11)' can't be established.
		ECDSA key fingerprint is SHA256:JGrKQGk9+m0eDIxj8Ttwk2oL5abYgUcCazjXSp36dxs.
		ECDSA key fingerprint is MD5:ec:b0:32:f5:c4:6d:68:07:03:f0:ad:7a:a6:a7:f9:47.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp02,172.16.238.11' (ECDSA) to the list of known hosts.
		steve@stapp02's password: 

[steve@stapp02 ~]$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for steve: 

4. Check if given packages are already  installed

[root@stapp02 ~]# rpm -qa|grep -e net-tools -e unzip

5. Run the puppet agent to pull configuration from puppet server 

[root@stapp02 ~]# puppet agent -tv
		Info: Using configured environment 'production'
		Info: Retrieving pluginfacts
		Info: Retrieving plugin
		Info: Retrieving locales
		Info: Caching catalog for stapp02.stratos.xfusioncorp.com
		Info: Applying configuration version '1666708349'
		Notice: /Stage[main]/Multi_package_node/Package[net-tools]/ensure: created
		Notice: /Stage[main]/Multi_package_node/Package[unzip]/ensure: created
		Notice: Applied catalog in 14.67 seconds

6. Validate the task by checking required packages installed

[root@stapp02 ~]# rpm -qa|grep -e net-tools -e unzip
		unzip-6.0-24.el7_9.x86_64
		net-tools-2.0-0.25.20131004git.el7.x86_64

[root@stapp02 ~]#

--------------------------------------------------------------------------------------------------------------------------------
Task 97:

--------------------------------------------------------------------------------------------------------------------------------
Task 98:

--------------------------------------------------------------------------------------------------------------------------------
Task 99:

--------------------------------------------------------------------------------------------------------------------------------
Task 100:

--------------------------------------------------------------------------------------------------------------------------------
Task 101:

--------------------------------------------------------------------------------------------------------------------------------
Task 102:
